{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## In this lab we are going to implement the following algorithms from the course:\n",
    " - policy evaluation\n",
    " - policy improvement\n",
    " - policy iteration\n",
    " - value iteration\n",
    " \n",
    "You can review these either from the course, or read Chapter 4 from the book Introduction to Reinforcement Learning by Sutton (attached to Files section in Teams)\n",
    " \n",
    "We'll work on the Frozen Lake environment: https://www.gymlibrary.dev/environments/toy_text/frozen_lake/\n",
    "\n",
    "Read the description, but look at their github implementation of the environment later, after implementing this lab.\n",
    "For now, follow the code in the cell below, function `runEpisode' to see how we load and interact with this environment\n",
    "Remember that all environments from gym have a similar structure and it is important to understand the API !\n",
    "\n",
    "### Note that in order to solve our problems using Bellman equations (DP method) as we do in this laboratory means two things:\n",
    " - We have full access to the model dynamics - which we do, as you see below with matrix P. \n",
    "   In the continuation of the course, as in most of the problems this information is not available because it is very difficult to obtain !\n",
    " - We can represent numerically all the states and actions. Is this feasible for a self driving car ?\n",
    " - However, when the above requirements can be satisfied, NOTE that the solutions are OPTIMAL !. Choose your algorithms wisely !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\peridot\\appdata\\roaming\\python\\python37\\site-packages (1.21.6)\n",
      "Requirement already satisfied: gym in c:\\users\\peridot\\.conda\\envs\\tf_cpu\\lib\\site-packages (0.24.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\peridot\\appdata\\roaming\\python\\python37\\site-packages (from gym) (4.11.4)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\peridot\\appdata\\roaming\\python\\python37\\site-packages (from gym) (1.21.6)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\peridot\\.conda\\envs\\tf_cpu\\lib\\site-packages (from gym) (0.0.6)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\peridot\\.conda\\envs\\tf_cpu\\lib\\site-packages (from gym) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\peridot\\appdata\\roaming\\python\\python37\\site-packages (from importlib-metadata>=4.8.0->gym) (4.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\peridot\\appdata\\roaming\\python\\python37\\site-packages (from importlib-metadata>=4.8.0->gym) (3.8.0)\n"
     ]
    }
   ],
   "source": [
    "# First ensure that we can install numpy and gym here then import them\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install gym\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### This is how we load the environment. We give it the name and the parameters (which are sent to the __init__ function of the respective class)\n",
    "### Try to switch the name of the map between 4x4 and 8x8 to see performance.\n",
    "### is_splippery = False means the environment is deterministic, while True means that doing an action \"a\" in state \"s\" can cause movement to different states \"s'\" ! (non-deterministic !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\",  map_name=\"4x4\", is_slippery=False)\n",
    "\n",
    "\"\"\"\n",
    "You can see from documentation that this environment contains three main things inside:\n",
    "\tP: nested dictionary \n",
    "\t    (simulates the  p(s',r | s, a) = the probability of being in state s, applying action a and landing in state s' with a reward of r)\n",
    "\t\tFrom gym.core.Environment:\n",
    "\t\tFor each pair of states in [1, nS] and actions in [1, nA], P[state][action] is a\n",
    "\t\ttuple of the form (probability, nextstate, reward, terminal) where\n",
    "\t\t\t- probability: float\n",
    "\t\t\t\tthe probability of transitioning from \"state\" to \"nextstate\" with \"action\"\n",
    "\t\t\t- nextstate: int\n",
    "\t\t\t\tdenotes the state we transition to (in range [0, nS - 1])\n",
    "\t\t\t- reward: int\n",
    "\t\t\t\teither 0 or 1, the reward for transitioning from \"state\" to\n",
    "\t\t\t\t\"nextstate\" with \"action\"\n",
    "\t\t\t- terminal: bool\n",
    "\t\t\t  True when \"nextstate\" is a terminal state (hole or goal), False otherwise\n",
    "\tnS: int\n",
    "\t\tnumber of states in the environment\n",
    "\tnA: int\n",
    "\t\tnumber of actions in the environment\n",
    "\t\tInside, they implement it with an enum:\n",
    "\t\tLEFT = 0\n",
    "        DOWN = 1\n",
    "        RIGHT = 2\n",
    "        UP = 3\n",
    "\"\"\"\n",
    "\n",
    "def runEpisode(env, policy, maxSteps=100):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      Environment to play on. Must have nS, nA, and P as\n",
    "      attributes.\n",
    "    Policy: np.array of shape [env.nS]\n",
    "      The action to take at a given state\n",
    "    \"\"\"\n",
    "    \n",
    "    # We count here the total\n",
    "    total_reward = 0\n",
    "    \n",
    "    # THis is how we reset the environment to an initial state, it returns the observation.\n",
    "    # As documented, in this case the observation is the state where the agent currently is positionaed, \n",
    "    #, which is a number in [0, nS-1]. We can use local function stateToRC to get the row and column of the agent\n",
    "    # The action give is in range [0, nA-1], check the enum defined above to understand what each number means\n",
    "    obs = env.reset() \n",
    "    for t in range(maxSteps):\n",
    "        # Draw the environment on screen\n",
    "        env.render() \n",
    "        # Sleep a bit between decisions\n",
    "        time.sleep(0.25)\n",
    "        \n",
    "        # Here we sample an action from our policy, we consider it deterministically at this point\n",
    "        action = policy[obs]\n",
    "        \n",
    "        # Hwere we interact with the enviornment. We give it an action to do and it returns back:\n",
    "        # - the new observation (observable state by the agent),\n",
    "        # - the reward of the action just made\n",
    "        # - if the simulation is done (terminal state)\n",
    "        # - last parameters is an \"info\" output, we are not interested in this one that's why we ignore the parameter\n",
    "        newObs, reward, done, _ = env.step(action)\n",
    "        print(f\"Agent was in state {obs}, took action {action}, now in state {newObs}\")\n",
    "        obs = newObs\n",
    "        \n",
    "        total_reward += reward\n",
    "        # Close the loop before maxSteps  if we are in a terminal state\n",
    "        if done:\n",
    "            break\n",
    "   \n",
    "    if not done:   \n",
    "        print(f\"The agent didn't reach a terminal state in {maxSteps} steps.\")\n",
    "    else:\n",
    "        print(f\"Episode reward: {total_reward}\")\n",
    "    env.render() # One last  rendering of the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 3 3 2 2 1 1 2 0 0 1 2 3 0 0]\n",
      "Agent was in state 0, took action 2, now in state 1\n",
      "Agent was in state 1, took action 0, now in state 0\n",
      "Agent was in state 0, took action 2, now in state 1\n",
      "Agent was in state 1, took action 0, now in state 0\n",
      "Agent was in state 0, took action 2, now in state 1\n",
      "Agent was in state 1, took action 0, now in state 0\n",
      "Agent was in state 0, took action 2, now in state 1\n",
      "Agent was in state 1, took action 0, now in state 0\n",
      "Agent was in state 0, took action 2, now in state 1\n",
      "Agent was in state 1, took action 0, now in state 0\n",
      "The agent didn't reach a terminal state in 10 steps.\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the code below to  create a random policy and see an episode in action.\n",
    "# Run it several times and see your agent in action with a random deterministic policy :)\n",
    "env.nA = 4\n",
    "env.nS = 16\n",
    "random_policy = np.random.choice(env.nA, size=env.nS)\n",
    "print(random_policy)\n",
    "runEpisode(env, random_policy, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md \n"
    }
   },
   "outputs": [],
   "source": [
    "### Now let's run the episode with this policy found by value iteration\n",
    "### Note that you may need to increase max number of Steps or run it several times if you select is_slippery !\n",
    "### Try to play with map namees and stochastic parameter is_slippery than run again all above cells including this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to improve policies\n",
      "PI step 1: \n",
      "Policy evaluation converged after 1 iterations\n",
      "PI step 2: \n",
      "Policy evaluation converged after 2 iterations\n",
      "PI step 3: \n",
      "Policy evaluation converged after 3 iterations\n",
      "PI step 4: \n",
      "Policy evaluation converged after 4 iterations\n",
      "PI step 5: \n",
      "Policy evaluation converged after 5 iterations\n",
      "PI step 6: \n",
      "Policy evaluation converged after 6 iterations\n",
      "PI step 7: \n",
      "Policy evaluation converged after 7 iterations\n",
      "Got a stable policy after 7 iterations!\n"
     ]
    }
   ],
   "source": [
    "def policy_evaluation(P, nS, nA, policy, gamma=0.9, tol=1e-3):\n",
    "    \"\"\"Evaluate the value function from a given policy.\n",
    "    Parameters\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    policy: np.array[nS]\n",
    "        The policy to evaluate. Maps states to actions, deterministic !\n",
    "    tol: float\n",
    "        Terminate policy evaluation when\n",
    "            max |value_function(s) - prev_value_function(s)| < tol\n",
    "    Returns\n",
    "    -------\n",
    "    value_function: np.ndarray[nS]\n",
    "        The value function of the given policy, where value_function[s] is\n",
    "        the value of state s\n",
    "    \"\"\"\n",
    "    # Init with 0 for all states, \n",
    "    # Remember that terminal states MUST have 0 always whatever you initialize them with here\n",
    "    value_function = np.zeros(nS) \n",
    "\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    maxChange = np.inf\n",
    "    numIters = 0\n",
    "    while maxChange > tol:\n",
    "        numIters += 1\n",
    "        maxChange = -np.inf\n",
    "        for s in range(nS):\n",
    "            a = policy[s] # We have a deterministic policy, no need to iterate over actions in this case\n",
    "            \n",
    "            # Let's check the next moves we get from starting in state s and applying action a\n",
    "            new_value_func = 0.0\n",
    "            for nextMove in P[s][a]:\n",
    "                probability, nextstate, reward, terminal = nextMove\n",
    "                new_value_func += probability * (reward + gamma * value_function[nextstate]) # if policy wouldn't be deterministic  we would have to multiply all this with probability given by each a, pi(a|s)\n",
    "                \n",
    "            maxChange = max(maxChange, abs(new_value_func - value_function[s]))\n",
    "            value_function[s] = new_value_func\n",
    "            \n",
    "    print(f\"Policy evaluation converged after {numIters} iterations\")\n",
    "        \n",
    "\t############################\n",
    "\n",
    "    return value_function\n",
    "\n",
    "def policy_improvement(P, nS, nA, value_from_policy, policy, gamma=0.9):\n",
    "    \"\"\"Given the value function from policy improve the policy.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    value_from_policy: np.ndarray\n",
    "        The value calculated from the policy\n",
    "    policy: np.array\n",
    "        The previous policy.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    new_policy: np.ndarray[nS]\n",
    "        An array of integers. Each integer is the optimal action to take\n",
    "        in that state according to the environment dynamics and the\n",
    "        given value function.\n",
    "    \"\"\"\n",
    "\n",
    "    new_policy = np.zeros(nS, dtype='int') # Default is left action\n",
    "\n",
    "\t############################\n",
    "\t# YOUR IMPLEMENTATION HERE #\n",
    "    \n",
    "    # Go through each state\n",
    "    for s in range(nS):\n",
    "        # Evaluate each actions\n",
    "        value_per_action = np.zeros(shape=(nA,))\n",
    "        for a in range(nA):\n",
    "            action_val = 0.0\n",
    "            # Get the model dynamics for this action and compute this action value\n",
    "            for nextMove in P[s][a]:\n",
    "                probability, nextstate, reward, terminal = nextMove\n",
    "                action_val += probability * (reward + gamma * value_from_policy[nextstate]) # Here we take one step ahead then use the estimated value function of the next step\n",
    "            \n",
    "            value_per_action[a] = action_val\n",
    "        \n",
    "        # Choose the best action in the given state \n",
    "        best_action = np.argmax(value_per_action)\n",
    "        new_policy[s] = best_action\n",
    "\n",
    "    ############################\n",
    "    return new_policy\n",
    "\n",
    "\n",
    "def policy_iteration(P, nS, nA, gamma=0.9, tol=10e-3):\n",
    "    \"\"\"Runs policy iteration.\n",
    "        You should call the policy_evaluation() and policy_improvement() methods to\n",
    "        implement this method.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        P, nS, nA, gamma:\n",
    "            defined at beginning of file\n",
    "        tol: float\n",
    "            tol parameter used in policy_evaluation()\n",
    "        Returns:\n",
    "        ----------\n",
    "        value_function: np.ndarray[nS]\n",
    "        policy: np.ndarray[nS]\n",
    "    \"\"\"\n",
    "\n",
    "    value_function = np.zeros(nS)\n",
    "    policy = np.zeros(nS, dtype=int)\n",
    "\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    print(\"Starting to improve policies\")\n",
    "    numIters = 0\n",
    "    while True:\n",
    "        numIters += 1\n",
    "        print(f\"PI step {numIters}: \")\n",
    "        value_function = policy_evaluation(P, nS, nA, policy, gamma, tol)\n",
    "        new_policy = policy_improvement(P, nS, nA, value_from_policy=value_function, policy=policy, gamma=gamma)\n",
    "        isPolicyStable = not np.any(new_policy != policy) \n",
    "        \n",
    "        if isPolicyStable:\n",
    "            break\n",
    "            \n",
    "        policy = new_policy\n",
    "    \n",
    "    print(f\"Got a stable policy after {numIters} iterations!\")\n",
    "    \n",
    "    ############################\n",
    "    return value_function, policy\n",
    "\n",
    "\n",
    "gamma = 0.9\n",
    "best_V, best_PI = policy_iteration(env.P, env.nS, env.nA, gamma=gamma, tol=10e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent was in state 0, took action 1, now in state 4\n",
      "Agent was in state 4, took action 1, now in state 8\n",
      "Agent was in state 8, took action 2, now in state 9\n",
      "Agent was in state 9, took action 1, now in state 13\n",
      "Agent was in state 13, took action 2, now in state 14\n",
      "Agent was in state 14, took action 2, now in state 15\n",
      "Episode reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Now let's run the episode with this policy found !\n",
    "runEpisode(env, policy=best_PI, maxSteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration converged after 7\n"
     ]
    }
   ],
   "source": [
    "# Now let's implement value iteration algorithm, which in general can converge faster !\n",
    "def value_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
    "    \"\"\"\n",
    "    Learn value function and policy by using value iteration method for a given\n",
    "    gamma and environment.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    tol: float\n",
    "        Terminate value iteration when\n",
    "            max |value_function(s) - prev_value_function(s)| < tol\n",
    "    Returns:\n",
    "    ----------\n",
    "    value_function: np.ndarray[nS]\n",
    "    policy: np.ndarray[nS]\n",
    "    \"\"\"\n",
    "\n",
    "    value_function = np.zeros(nS)\n",
    "    policy = np.zeros(nS, dtype=int)\n",
    "\t############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    \n",
    "    # Step 1: Value iteration in place. Its output is the optimal policy according to the Bellman equation\n",
    "    maxChange = np.inf\n",
    "    numIters = 0\n",
    "    while maxChange > tol:\n",
    "        numIters += 1\n",
    "        maxChange = 0.0\n",
    "        for s in range(nS):\n",
    "            # Update the value of s by moving towards the action that maximizes the value according to the model dynamics\n",
    "            bestActionValue = -np.inf\n",
    "            for a in range(nA):\n",
    "                value_for_thisAction = 0.0\n",
    "                \n",
    "                # Check model dynamics from going from state s and action a\n",
    "                for nextMove in P[s][a]:\n",
    "                    probability, nextstate, reward, terminal = nextMove\n",
    "                    value_for_thisAction += probability * (reward + gamma * value_function[nextstate])\n",
    "                    \n",
    "                if value_for_thisAction > bestActionValue:\n",
    "                    bestActionValue = value_for_thisAction\n",
    "                \n",
    "            maxChange = max(maxChange, abs(value_function[s] - bestActionValue))\n",
    "            value_function[s] = bestActionValue\n",
    "            \n",
    "    print(f\"Value iteration converged after {numIters}\")\n",
    "    \n",
    "    # Now extract the best policy from the value computed above\n",
    "    policy = policy_improvement(P, nS, nA, value_function, policy, gamma)\n",
    "\n",
    "    ############################\n",
    "    return value_function, policy\n",
    "\n",
    "gamma = 0.9\n",
    "best_value, best_policy = value_iteration(env.P, env.nS, env.nA, gamma=gamma, tol=10e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Now let's run the episode with this policy found by value iteration\n",
    "### Note that you may need to increase max number of Steps or run it several times if you select is_slippery !\n",
    "### Try to play with map names and stochastic parameter is_slippery than run again all above cells including this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent was in state 0, took action 1, now in state 4\n",
      "Agent was in state 4, took action 1, now in state 8\n",
      "Agent was in state 8, took action 2, now in state 9\n",
      "Agent was in state 9, took action 1, now in state 13\n",
      "Agent was in state 13, took action 2, now in state 14\n",
      "Agent was in state 14, took action 2, now in state 15\n",
      "Episode reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "runEpisode(env, policy=best_PI, maxSteps=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
